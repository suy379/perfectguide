{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분석(Text Mining)\n",
    "- 텍스트(비정형 데이터)를 기반으로 여러 가지 분석을 수행하는 방법을 통칭. 다른 말로는 NLP라고도 함.(둘이 살짝 차이 있음)\n",
    "\n",
    "<텍스트 분석의 종류>\n",
    "- 1)텍스트 분류(Text Classification): 문서가 '어떤 특정 분류(카테고리)'에 속하는지를 분류하는 것. ->지도학습\n",
    "- 2)감성 분석(Sentiment Analysis): 텍스트에서 나타나는 '주관적인 요소'를 분석하는 기법. 긍/부정 리뷰, 소셜 미디어 감정분석 등. ->지도학습, 비지도학습\n",
    "- 3)텍스트 요약(Summarziation): 텍스트 내 중요한 주제나 중심 사상을 추출. 가장 대표적인 것이 '토픽 모델링' ->비지도학습\n",
    "- 4)텍스트 군집화(Clustering) & 유사도 측정: 비슷한 유형의 문서에 대해 군집화 수행. 문서들 간 유사도를 측정해 비슷한 문서끼리 모으는 방법. ->비지도학습(텍스트 분류의 비지도학습화)\n",
    "\n",
    "<텍스트 분석의 순서>\n",
    "- 1)텍스트 전처리- 사전에 클렌징, 대소문자 변경, 특수문자 삭제, 토큰화, 스탑워즈 제거, 어근 추출 등의 텍스트 정규화 작업\n",
    "- 2)피처 벡터화/추출 - 앞에서 가공된 전처리된 데이터에서 피처를 추출하고 벡터 값을 할당하는 방식. BOW와 Word2Vec 방법을 활용, 여기선 BOW 활용.\n",
    " ==>왜냐면 우리의 ML 모델들은 숫자형 벡터 값만을 인식할 수 있기 때문!\n",
    "- 3)텍스트 분석- ML 모델 생성 + 학습/예측/평가\n",
    "- (1,2번을 완수하면 정형 데이터로 바뀌어 ML을 돌릴 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 텍스트 사전 작업(텍스트 전처리, 정규화)\n",
    "- 텍스트 분석을 수행하기에 앞서, 텍스트는 \"비정형 데이터\"이기 때문에 곧바로 ML 모델을 돌릴 수가 없음. 반드시 전처리가 필요\n",
    "- 텍스트 전처리: 사전에 클렌징, 대소문자 변경, 특수문자 삭제, 토큰화, 스탑워즈 제거, 어근 추출 등의 텍스트 정규화 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <텍스트 토큰화>\n",
    "- 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 토큰화: 전부 하나로 뭉쳐있는 텍스트를 문장의 마지막을 뜻하는 끝 기호(마침표, 개행문자(\\n))를 이용하여 문장을 토큰화하는 것(자르는 것)\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room.  \\\n",
    "              You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.' #모두 하나로 뭉쳐있는 텍스트->문장 단위로 자르기!\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n",
      "The Matrix is everywhere its all around us, here even in this room.\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences)) #반환된 문장은 리스트 형태\n",
    "print(len(sentences))\n",
    "print(sentences[0]) #첫번째 문장을 가져오면 뭉친 텍스트 중 첫번째 문장 나옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 토큰화: 하나로 뭉쳐있는 문장을 -> 끝 기호를 이용하여 단어로 토큰화하는 것(자르는 것)\n",
    "    ##사실 문장이 가지는 의미가 중요한 경우에만 문장 토큰화를 하고, 보통은 단어 순서가 중요하지 않으므로 단어 토큰화만 사용.\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "15\n",
      "Matrix\n"
     ]
    }
   ],
   "source": [
    "print(type(words)) #아까와 똑같이 list로 반환\n",
    "print(len(words))\n",
    "print(words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####문서 -> 문장 -> 단어 토큰화를 수행하는 함수\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    #문장 토큰화\n",
    "    sent = sent_tokenize(text) #반환된 sent는 리스트\n",
    "    #단어 토큰화\n",
    "    word = [word_tokenize(s) for s in sent] #word = word_tokenize(sent) 이렇게 사용하면 에러!\n",
    "                                                    ##왜냐면 문서를 문장 하나하나로 토큰화한 후 첫번째 문장부터 단어 토큰화를 수행하기 때문.\n",
    "                                                              #이 방식은 for loop를 사용하지 않고도, (list나 array 객체에 대해서만 사용가능) for loop를 이용하는 것처럼 사용!\n",
    "    return word #반환된 word도 리스트.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "words_token = tokenize_text(text_sample)\n",
    "print(type(words_token))\n",
    "print(words_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_token) #문장의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <n-gram 방법>\n",
    "- ngrams(단어, 숫자)를 입력하면 해당 단어를 숫자만큼 재반복, 숫자만큼씩 끊어줌.(단어 토큰화의 경우 단어들의 문맥적인 의미를 무시할 수 있기 때문에 사용)\n",
    "- 그렇지만 사실 문맥 의미를 이해하는 데 큰 도움은 안됨,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object ngrams at 0x0000015A624909A8>\n",
      "[('The', 'Matrix', 'is', 'everywhere'), ('Matrix', 'is', 'everywhere', 'its'), ('is', 'everywhere', 'its', 'all'), ('everywhere', 'its', 'all', 'around'), ('its', 'all', 'around', 'us'), ('all', 'around', 'us', ','), ('around', 'us', ',', 'here'), ('us', ',', 'here', 'even'), (',', 'here', 'even', 'in'), ('here', 'even', 'in', 'this'), ('even', 'in', 'this', 'room'), ('in', 'this', 'room', '.')]\n",
      "<class 'list'>\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "all_ngrams = ngrams(words, 4)\n",
    "print(all_ngrams) #전용 객체로 반환되므로\n",
    "ngrams = [ngram for ngram in all_ngrams] #얘로 출력해줘야 함 (리스트로)\n",
    "print(ngrams)\n",
    "print(type(ngrams))\n",
    "print(len(ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <스탑워즈 제거>\n",
    "- 스탑워즈(stopwords): 빈번하게 등장하지만 아무런 문맥상의 의미가 없는 단어.\n",
    "- NLTK에서는 스탑워즈를 이미 제공중 ->다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suyn3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') #다운로드 완료(다운로드는 1번만 하면 됨!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#제공되는 영어 stopwords\n",
    "print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']\n",
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "#아까 수행한 함수에서 반환된 'words_token'에 대해 스탑워즈 제거, 소문자화 함수\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english') #스탑워즈 지정\n",
    "all_tokens=[]\n",
    "\n",
    "for sent in words_token: #아까 반환된 'words_token'은 리스트로, 들어있는 것은 3개의 문장\n",
    "    filtered_words = []\n",
    "    for word in sent:\n",
    "        #소문자 변환\n",
    "        word = word.lower()\n",
    "        #토큰화된 개별 단어(word)가 스탑워즈 단어에 포함되지 않으면 filtered_words 에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "     \n",
    "    #for문을 벗어난후\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(filtered_words) #얘는 출력될 시 맨 마지막 sent의 word만 남아있으니까\n",
    "print(all_tokens) #전체반환을 위해 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(type(all_tokens))\n",
    "print(len(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.', 'see', 'window', 'television', '.', 'feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']\n",
      "<class 'list'>\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "##번외) 위의 all_tokens는 리스트 개수가 문장이라 3개고, 각 문장 내에 단어들이 들어가 있음. \n",
    "        #만일 문장 내 단어들을 다 빼서 하나의 리스트로 만들고 싶다면 밑처럼 이용!\n",
    "a = []\n",
    "\n",
    "for ind, value in enumerate(all_tokens):\n",
    "    for i in range(len(value)): #len(value)대신 len(all_tokens[ind]) 도 됨\n",
    "        word=all_tokens[ind][i]\n",
    "        a.append(word)\n",
    "\n",
    "print(a)\n",
    "print(type(a))\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <Stemming & Lemmatization>\n",
    "- 문법적 또는 의미적으로 변화하는 단어의 원형을 찾아주는 기법.\n",
    "- stemming보단 lemmatization이 좀 더 단어의 원형을 잘 찾아주므로 더 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "#객체\n",
    "stemmer=LancasterStemmer()\n",
    "#적용-> 객체.stem('단어')\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest')) #결과: work 빼고 전부 단어 원형을 잘 찾지 못했음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woring work work\n",
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#객체\n",
    "lemma = WordNetLemmatizer()\n",
    "#적용-> 객체.lemmatize('단어', '품사') ##v:동사, a:형용사.\n",
    "print(lemma.lemmatize('woring','v'), lemma.lemmatize('works','v'), lemma.lemmatize('worked','v'))\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a')) #결과: 앞의 stemming에서 잘 찾지 못한 단어들의 원형을 모두 잘 찾았음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BOW(Bag Of Words)\n",
    "- 앞의 1번을 수행한 전처리된 데이터에서, \"피처를 추출하고 벡터 값을 할당\"하는 방법 중 하나. (Word2Vec 방법도 있음)\n",
    "- 문서가 가지는 모든 단어의 문맥, 순서를 무시하고 Bag 안에 넣고, 단어의 \"빈도 값\"에 따라 피처 값을 추출하는 모델.\n",
    "- 이것을 수행하면 row: 문서들(m개), col: 단어들(n개), value: 문서 속에 단어들이 얼마나 등장하는지의 값. (총 mxn 행렬, 0의 값이 아주 많은 희소행렬)이 출력\n",
    "\n",
    "### BOW로 피처 벡터화 하는 방식- \"기준: value에 무엇이 들어가는가?\"\n",
    "- 다음 2가지 방식은 모두 사이킷런에서 제공하며, 반환하는 희소 행렬은 CSR 형식으로 만든다.\n",
    "- 그리고 반환되는 희소행렬을 갖고 ML 모델을 만들어 적용하는 3번째 단계로 고고씽!\n",
    "\n",
    "##### 1. 카운트 벡터(CountVectorizer)\n",
    "- value에는 문서 속에 단어들이 등장하는 횟수만 출력.\n",
    "\n",
    "##### 2. TF-IDF 벡터(TfidfVectorizer)\n",
    "- value에, 문서 속에 단어들이 등장하는 횟수가 많으면 가중치를 주되, 전체 문서 전체에서도 많이 등장하면 패널티를 부여.(가중치 값을 좀 깎음)\n",
    "- 왜냐하면 전체 문서에도 다 많이 등장하는 것은 stopwords와 비슷한 역할이기 때문. 그래서 피처벡터화 할 때는 TF-IDF 방식을 많이 사용함.\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 희소 행렬 만드는 방식(COO, CSR 방식)\n",
    "- 1. COO 형식으로 만든 희소 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#희소 행렬-COO 형식\n",
    "dense = np.array( [[3,0,1], [0,2,0]] ) #밀집행렬\n",
    "#0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "#이것의 행위치(row_pos) , 열위치(col_pos) 지정\n",
    "row_pos= np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "#COO 형식의 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 3 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sparse_coo를 toarray로 만들어보면 원본 행렬 도출\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. CSR 형식으로 만든 희소 행렬(CSR: Compressed Sparse Row)\n",
    "- COO 형식을 사용 시 행 위치를 기록하는 row_pos가 길어질 수 있는데, 심지어 같은 값만 연속해서 길어질 수 있으므로 이것을 보완하여, 고유한 값의 위치만 적는 방식으로 메모리를 줄임\n",
    "- 이름에서도 느껴지는.. CSR: Sparse \"Row\"를 압축했다는 뜻."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#처음엔 COO 형식으로\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]]) #원본\n",
    "\n",
    "#0이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "#행위치, 열위치 array 생성\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "#COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "print(sparse_coo.toarray()) #다시 원본 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#COO 형식을 보완한 CSR 형식\n",
    "\n",
    "#행위치배열을 고유한 값의 시작 위치 인덱스로만 간결하게 생성\n",
    "row_pos_new = np.array([0,2,7,9,10,12,13]) #마지막은 총 개수\n",
    "\n",
    "#CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_new))\n",
    "print(sparse_csr.toarray()) #다시 원본 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실제 사용 시에는 이렇게 직접 위치 array를 만들 필요 없고, 그냥 sparse.coo_matrix(원본행렬), sparse.csr_matrix(원본행렬) 넣으면 됨.\n",
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "#여기서 만든 객체 coo, csr을 이제 ML 모델에 적용하는 것이고 얘네를 출력할 수는 없음. 뒤에 .toarray() 적용하면 원본이 출력됨으로써 잘 수행했음을 알수있음.\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 5],\n",
       "       [1, 4, 0, 3, 2, 5],\n",
       "       [0, 6, 0, 3, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 7, 0, 8],\n",
       "       [1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 5],\n",
       "       [1, 4, 0, 3, 2, 5],\n",
       "       [0, 6, 0, 3, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 7, 0, 8],\n",
       "       [1, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사이킷런 CountVectorizer 테스트 : 희소 행렬 만드는 것은 CSR 행렬 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.                   You can see it out your window or on your television.                   You feel it when you go to work, or go to church or pay your taxes.', 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'] \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "text_sample_01 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                  You can see it out your window or on your television. \\\n",
    "                  You feel it when you go to work, or go to church or pay your taxes.'\n",
    "text_sample_02 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe\\\n",
    "                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "text=[]\n",
    "text.append(text_sample_01); text.append(text_sample_02)\n",
    "print(text,\"\\n\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CountVectorizer객체 생성 후 fit(), transform()으로 텍스트에 대한 feature vectorization 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(2, 51)\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t3\n",
      "  (0, 25)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 40)\t2\n",
      "  :\t:\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t2\n",
      "  (1, 27)\t2\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 35)\t2\n",
      "  (1, 38)\t4\n",
      "  (1, 40)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 47)\t1\n",
      "  (1, 49)\t7\n",
      "  (1, 50)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#객체\n",
    "cnt_vect = CountVectorizer() #아무런 하이퍼 파라미터도 주지 않음\n",
    "#학습\n",
    "cnt_vect.fit(text)\n",
    "#적용\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "#결과\n",
    "print(type(ftr_vect)) #csr 기반 생성된 희소행렬\n",
    "print(ftr_vect.shape) #row는 0과 1밖에 없으므로 전체 행 수가 2임. 왜냐면 앞에서 text_sample이 2개였으므로.\n",
    "print(ftr_vect) #(row(문서번호=m), col(단어번호=n)), value(횟수 값) , #당연히 0이 아닌 값만 대상으로 하므로 value가 모두 1 이상. #(2x51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderland': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n"
     ]
    }
   ],
   "source": [
    "#객체.vocabulary_ : countvector로 만들어진 객체가 가진 단어들, 등장횟수\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CountVectorizer() 객체에 하이퍼 파라미터 튜닝 시 달라지는 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 5)\n",
      "{'window': 4, 'pill': 1, 'wake': 2, 'believe': 0, 'want': 3}\n",
      "  (0, 4)\t1\n",
      "  (1, 0)\t2\n",
      "  (1, 1)\t2\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n"
     ]
    }
   ],
   "source": [
    "#하이퍼 파라미터 튜닝- max_features, stop_words \n",
    "cnt_vect = CountVectorizer(max_features=5, stop_words='english') #5개 단어만 출력(n=5)\n",
    "#학습, 적용\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "#결과\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)\n",
    "print(ftr_vect) #앞서 그냥 했던 것보다 많이 줄음(2x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 201)\n",
      "{'the': 129, 'matrix': 77, 'is': 66, 'everywhere': 40, 'its': 74, 'all': 0, 'around': 11, 'us': 150, 'here': 51, 'even': 37, 'in': 59, 'this': 140, 'room': 106, 'you': 174, 'can': 25, 'see': 109, 'it': 69, 'out': 90, 'your': 193, 'window': 165, 'or': 83, 'on': 80, 'television': 126, 'feel': 43, 'when': 162, 'go': 46, 'to': 143, 'work': 171, 'church': 28, 'pay': 93, 'taxes': 125, 'the matrix': 132, 'matrix is': 78, 'is everywhere': 67, 'everywhere its': 41, 'its all': 75, 'all around': 1, 'around us': 12, 'us here': 151, 'here even': 52, 'even in': 38, 'in this': 60, 'this room': 141, 'room you': 107, 'you can': 177, 'can see': 26, 'see it': 110, 'it out': 70, 'out your': 91, 'your window': 199, 'window or': 166, 'or on': 86, 'on your': 81, 'your television': 197, 'television you': 127, 'you feel': 179, 'feel it': 44, 'it when': 72, 'when you': 163, 'you go': 181, 'go to': 47, 'to work': 148, 'work or': 172, 'or go': 84, 'to church': 146, 'church or': 29, 'or pay': 88, 'pay your': 94, 'your taxes': 196, 'the matrix is': 133, 'matrix is everywhere': 79, 'is everywhere its': 68, 'everywhere its all': 42, 'its all around': 76, 'all around us': 2, 'around us here': 13, 'us here even': 152, 'here even in': 53, 'even in this': 39, 'in this room': 61, 'this room you': 142, 'room you can': 108, 'you can see': 178, 'can see it': 27, 'see it out': 111, 'it out your': 71, 'out your window': 92, 'your window or': 200, 'window or on': 167, 'or on your': 87, 'on your television': 82, 'your television you': 198, 'television you feel': 128, 'you feel it': 180, 'feel it when': 45, 'it when you': 73, 'when you go': 164, 'you go to': 182, 'go to work': 49, 'to work or': 149, 'work or go': 173, 'or go to': 85, 'go to church': 48, 'to church or': 147, 'church or pay': 30, 'or pay your': 89, 'pay your taxes': 95, 'take': 121, 'blue': 22, 'pill': 96, 'and': 3, 'story': 118, 'ends': 34, 'wake': 153, 'bed': 14, 'believe': 17, 'whatever': 159, 'want': 156, 'red': 103, 'stay': 115, 'wonderland': 168, 'show': 112, 'how': 56, 'deep': 31, 'rabbit': 100, 'hole': 54, 'goes': 50, 'you take': 187, 'take the': 122, 'the blue': 130, 'blue pill': 23, 'pill and': 97, 'and the': 6, 'the story': 138, 'story ends': 119, 'ends you': 35, 'you wake': 189, 'wake in': 154, 'in your': 64, 'your bed': 194, 'bed and': 15, 'and you': 8, 'you believe': 175, 'believe whatever': 18, 'whatever you': 160, 'you want': 191, 'want to': 157, 'to believe': 144, 'believe you': 20, 'the red': 136, 'red pill': 104, 'you stay': 185, 'stay in': 116, 'in wonderland': 62, 'wonderland and': 169, 'and show': 4, 'show you': 113, 'you how': 183, 'how deep': 57, 'deep the': 32, 'the rabbit': 134, 'rabbit hole': 101, 'hole goes': 55, 'you take the': 188, 'take the blue': 123, 'the blue pill': 131, 'blue pill and': 24, 'pill and the': 98, 'and the story': 7, 'the story ends': 139, 'story ends you': 120, 'ends you wake': 36, 'you wake in': 190, 'wake in your': 155, 'in your bed': 65, 'your bed and': 195, 'bed and you': 16, 'and you believe': 9, 'you believe whatever': 176, 'believe whatever you': 19, 'whatever you want': 161, 'you want to': 192, 'want to believe': 158, 'to believe you': 145, 'believe you take': 21, 'take the red': 124, 'the red pill': 137, 'red pill and': 105, 'pill and you': 99, 'and you stay': 10, 'you stay in': 186, 'stay in wonderland': 117, 'in wonderland and': 63, 'wonderland and show': 170, 'and show you': 5, 'show you how': 114, 'you how deep': 184, 'how deep the': 58, 'deep the rabbit': 33, 'the rabbit hole': 135, 'rabbit hole goes': 102}\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 42)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 44)\t1\n",
      "  (0, 45)\t1\n",
      "  (0, 46)\t2\n",
      "  (0, 47)\t2\n",
      "  (0, 48)\t1\n",
      "  (0, 49)\t1\n",
      "  :\t:\n",
      "  (1, 156)\t1\n",
      "  (1, 157)\t1\n",
      "  (1, 158)\t1\n",
      "  (1, 159)\t1\n",
      "  (1, 160)\t1\n",
      "  (1, 161)\t1\n",
      "  (1, 168)\t1\n",
      "  (1, 169)\t1\n",
      "  (1, 170)\t1\n",
      "  (1, 174)\t7\n",
      "  (1, 175)\t1\n",
      "  (1, 176)\t1\n",
      "  (1, 183)\t1\n",
      "  (1, 184)\t1\n",
      "  (1, 185)\t1\n",
      "  (1, 186)\t1\n",
      "  (1, 187)\t2\n",
      "  (1, 188)\t2\n",
      "  (1, 189)\t1\n",
      "  (1, 190)\t1\n",
      "  (1, 191)\t1\n",
      "  (1, 192)\t1\n",
      "  (1, 193)\t1\n",
      "  (1, 194)\t1\n",
      "  (1, 195)\t1\n"
     ]
    }
   ],
   "source": [
    "#하이퍼 파라미터 튜닝- ngram\n",
    "cnt_vect = CountVectorizer(ngram_range=(1,3))\n",
    "#학습, 적용\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "#결과\n",
    "print(type(ftr_vect), ftr_vect.shape) #출력되는 개수가 더 늘어남(2x201)\n",
    "print(cnt_vect.vocabulary_)\n",
    "print(ftr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 5)\n",
      "{'pill': 2, 'story': 3, 'bed': 0, 'believe': 1, 'story ends': 4}\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 2)\t2\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n"
     ]
    }
   ],
   "source": [
    "#하이퍼 파라미터 튜닝 -모두\n",
    "cnt_vect = CountVectorizer(max_features=5, stop_words='english', ngram_range=(1,3))\n",
    "#학습, 적용\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "#결과\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_) #max_features 제한 때문에 크기는 2x5로 같지만 출력되는 vocabulary가 다름\n",
    "print(ftr_vect) #앞서 그냥 했던 것보다 많이 줄음(2x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 BOW방식을 적용하여 반환되는 희소행렬(ftr_vect)을 갖고 ML 모델을 만들어 적용하는 3번째 단계로 고고씽!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
